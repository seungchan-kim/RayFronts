"""Semantic segmentation evaluation script.

Check the SemSegEvalConfig class for all the parameters that can be set in the
config file specific to semantic segmentation evaluation.

Only works with Rerun visualizer or no visualizer since it uses rerun specific
functionality.

Typical usage:
  python semseg_eval.py \
    --config-dir experiments/semseg_configs/ --config-name naradio_replica --multirun
"""
from typing import Tuple, Union
import logging
from dataclasses import dataclass
import os
import sys
from collections import OrderedDict
import shutil

import torch
import rerun as rr
import hydra
from hydra.core.config_store import ConfigStore
from omegaconf import OmegaConf

import eval_utils

sys.path.insert(
    0, os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
)
from rayfronts import mapping, geometry3d as g3d, image_encoders, feat_compressors

logger = logging.getLogger(__name__)

@dataclass
class SemSegEvalConfig:
  """Configuration for semantic segmentation evaluation."""

  # Where to cache and output results of evaluation
  eval_out: str = "eval_out"

  # Do not use cached results if they exist. Will still write to cache.
  no_caching: bool = False

  # K nearest neighbor value for aligning voxels. Set to 0 to impose a 1 to 1
  # mapping (0 distance)
  k: int = 0

  # Prompt denoising introduced in MaskClip+, removes the prompt with target
  # class if its class confidence at all spatial locations is less than a
  # threshold.
  prompt_denoising_thresh: float = 0.5

  # Threshold after which a prediction is made. Otherwise will be set to ignore.
  prediction_thresh: float = 0.1

  # Classes to ignore in evaluation
  classes_to_ignore: Tuple[str] = tuple()

  # Classes that should be evaluated
  classes_to_eval: Tuple[str] = tuple()

  # Chunk size to compute cos similarity. Reduce if getting OOM error.
  chunk_size: int = 10000

  # Predict and semantically segment 2D feature maps for visualization.
  vis_2d_semseg: bool = False

  # How many frames should pass before eval ? Set to -1 to only eval at the end.
  online_eval_period: int = -1

  # Whether to log all label voxels / rays as one layer in rerun or split based
  # on class.
  split_pred_vis: bool = False

  # Load external ground truth 3D voxels from a file instead of lifting from 2D
  # images.
  load_external_gt: bool = False

  # Load external semantic segmentation predictions from a file instead of
  # computing them. Useful to evaluate on results generated by other codebases.
  load_external_pred: bool = False

cs = ConfigStore.instance()
cs.store(name="extras", node=SemSegEvalConfig)

class SemSegEval:
  def __init__(self, cfg):
    self.cfg = cfg

    if cfg.online_eval_period > 0 and cfg.load_external_pred:
      raise ValueError(
        "Cannot use online evaluation with external predictions.")

    self.store_output = cfg.eval_out is not None and len(cfg.eval_out) > 0

    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Init dataset and remap classes with white and black lists
    self.dataset = hydra.utils.instantiate(cfg.dataset)

    self.dataset._init_semseg_mappings(
      self.dataset._cat_id_to_name, cfg.classes_to_eval, cfg.classes_to_ignore)

    logger.info(self.dataset._cat_index_to_name)
    self.num_classes = len(self.dataset._cat_index_to_id)

    # Init the visualizer
    intrinsics_3x3 = self.dataset.intrinsics_3x3

    self.vis = None
    if hasattr(cfg, "vis") and cfg.vis is not None:
      assert "Rerun" in cfg.vis["_target_"], \
        "This script only supports Rerun visualizer"

      base_point_size = None
      if "vox_size" in cfg.mapping:
        base_point_size = cfg.mapping.vox_size / 2
      self.vis = hydra.utils.instantiate(cfg.vis, intrinsics_3x3=intrinsics_3x3,
                                         base_point_size=base_point_size,
                                         split_label_vis=cfg.split_pred_vis,
                                         device=self.device)
      rr.log("/", rr.AnnotationContext(
        rr.AnnotationInfo(id=x, label=y) for x,y
        in enumerate(self.dataset._cat_index_to_name)
      ))

    self.dataloader = torch.utils.data.DataLoader(
      self.dataset, batch_size=cfg.batch_size)

    # What properties of the config render the cache invalid if changed.
    self.cache_validity_props = {
      "semseg_gt": ["classes_to_ignore", "classes_to_eval", "seed",
                    "mapping.vox_size", "mapping.vox_accum_period",
                    "mapping.max_pts_per_frame", "dataset"],
      "map": ["seed", "mapping", "dataset", "encoder", "eval_period"],
      "text_embeds": ["seed", "encoder", "classes_to_ignore",
                      "classes_to_eval", "feat_compressor",
                      "querying.compressed"],
    }
    t = self.cache_validity_props
    t["semseg_preds"] = t["text_embeds"] + t["semseg_gt"] + t["map"] + \
      ["prompt_denoising_thresh", "prediction_thresh"]

    scene_name = cfg.dataset.scene_name.replace("/", "_")
    cache_dataset_dir = os.path.join(cfg.eval_out,
                                     self.dataset.__class__.__name__)
    self.cache_scene_dir = os.path.join(cache_dataset_dir, scene_name)
    self.cache_cfg_fn = os.path.join(self.cache_scene_dir, "cfg.yaml")
    self.cache_cfg = None
    if not os.path.exists(self.cache_scene_dir):
      os.makedirs(self.cache_scene_dir)
    elif os.path.exists(self.cache_cfg_fn):
      self.cache_cfg = OmegaConf.load(self.cache_cfg_fn)

    self.cache_file_paths = {
      "semseg_gt": os.path.join(self.cache_scene_dir, "semseg_gt.pt"),
      "map": os.path.join(self.cache_scene_dir, "map.pt"),
      "text_embeds": os.path.join(self.cache_scene_dir, "text_embeds.pt"),
      "semseg_preds": os.path.join(self.cache_scene_dir, "semseg_preds.pt"),
    }

  def cache_is_valid(self, key: str):
    """Check if the cache is valid for the given key.

    Args:
      key: The key to check the cache validity for.

    Returns:
      True if the cache is valid, False otherwise.
    """
    if self.cfg.no_caching or self.cache_cfg is None:
      return False
    if not os.path.exists(self.cache_file_paths[key]):
      return False

    # Check if any of the properties that invalidate the cache have changed
    for p in self.cache_validity_props[key]:
      if OmegaConf.select(self.cache_cfg, p) != OmegaConf.select(self.cfg, p):
        return False

    return True

  def load_externel_semseg_gt(self):
    """Load external ground truth 3D voxels."""
    fn = os.path.join(self.cache_scene_dir, "external_semseg_gt.pt")
    if not os.path.exists(fn):
      logger.fatal("Could not find external ground truth from '%s'", fn)
      sys.exit(1)
    logger.info("Loading external ground truth 3D voxels from '%s'...", fn)
    d = torch.load(fn, weights_only=True)
    gt_xyz = d["semseg_gt_xyz"].to(self.device)
    gt_label = d["semseg_gt_label"].to(self.device)
    gt_label[gt_label < 0] = 0
    gt_label = self.dataset._cat_id_to_index[gt_label]
    gt_onehot = torch.nn.functional.one_hot(gt_label, self.num_classes)
    gt_xyz, gt_onehot = g3d.pointcloud_to_sparse_voxels(
      gt_xyz, self.cfg.mapping.vox_size, gt_onehot, aggregation="sum")
    return gt_xyz, torch.argmax(gt_onehot, dim=-1)

  def compute_semseg_gt(self):
    """Compute the semantic segmentation ground truth from the dataset.
    Returns:
      Tuple of (semseg_gt_xyz, semseg_gt_label) 
      semseg_gt_xyz: (Nx3) float tensor representing the voxel centers.
      semseg_gt_label: (N) long tensor representing class indices.
    """

    names = self.dataset.cat_index_to_name[1:]
    gt_encoder = image_encoders.GTEncoder(classes=names)

    # TODO: Have a better visualizer of class logits.
    if self.vis is not None:
      prev_feat_compressor = self.vis.feat_compressor
      self.vis.feat_compressor = feat_compressors.PcaCompressor(3)

    semseg_gt_lifter = mapping.SemanticVoxelMap(
      self.dataset.intrinsics_3x3, None, self.vis,
      max_pts_per_frame=self.cfg.mapping.max_pts_per_frame,
      vox_size=self.cfg.mapping.vox_size,
      vox_accum_period=self.cfg.mapping.vox_accum_period,
      encoder=gt_encoder)

    logger.info("Lifting 2D ground truth to 3D voxels...")
    for i, batch in enumerate(self.dataloader):
      rgb_img = batch["rgb_img"].cuda()
      depth_img = batch["depth_img"].cuda()
      pose_4x4 = batch["pose_4x4"].cuda()
      semseg_img = batch["semseg_img"].cuda()
      if self.vis is not None:
        if i % self.cfg.vis.pose_period == 0:
          self.vis.log_pose(batch["pose_4x4"][-1])
        if i % self.cfg.vis.input_period == 0:
          self.vis.log_img(batch["rgb_img"][-1].permute(1,2,0))
          self.vis.log_depth_img(batch["depth_img"][-1].squeeze())
          self.vis.log_label_img(batch["semseg_img"][-1].squeeze())

      # Convert to onehot encoded
      semseg_onehot = torch.nn.functional.one_hot(
        semseg_img, self.num_classes)
      semseg_onehot = semseg_onehot.squeeze(1).permute(0, 3, 1, 2)

      semseg_gt_lifter.process_posed_rgbd(
        rgb_img, depth_img, pose_4x4, feat_img=semseg_onehot.float())

      if self.vis is not None:
        if i % self.cfg.vis.map_period == 0:
          semseg_gt_lifter.vis_map()

        self.vis.step()

    text_embeds = gt_encoder.encode_labels(names)
    semseg_gt_xyz = semseg_gt_lifter.global_vox_xyz
    semseg_gt_label = eval_utils.compute_semseg_preds(
      semseg_gt_lifter.global_vox_feat, text_embeds,
      0, 0.1, self.cfg.chunk_size)

    if self.vis is not None:
      self.vis.feat_compressor = prev_feat_compressor

    return semseg_gt_xyz, semseg_gt_label

  def compute_text_embeds(self):
    """Compute text embeddings for the dataset categories.
    Returns:
      A float tensor of shape (num_classes, embed_dim) containing the text
      embeddings for each category.
    """
    logger.info("Generating text embeddings...")
    names = self.dataset.cat_index_to_name[1:]

    if self.cfg.querying.text_query_mode == "labels":
      text_embeds = self.encoder.encode_labels(names)
    elif self.cfg.querying.text_query_mode == "prompts":
      text_embeds = self.encoder.encode_prompts(names)
    else:
      raise ValueError("Invalid query type")

    if (self.feat_compressor is not None and
        self.cfg.querying.compressed):
      if not self.feat_compressor.is_fitted():
        logger.warning("The feature compressor was not fitted. "
                       "Will try to fit to text features which may fail.")
        self.feat_compressor.fit(text_embeds)

      text_embeds = self.feat_compressor.compress(text_embeds)

    return text_embeds

  def mapping_loop(self, mapper, text_embeds=None):
    """Loop to compute map features from the dataset. Yields after every frame.

    Yields:
      Tuple of (feats_xyz, feats_feats) where feats_xyz are the voxel centers
      and feats_feats are the features at those voxels.
    """
    logger.info("Computing map features...")
    for i, batch in enumerate(self.dataloader):
      torch.cuda.empty_cache()
      rgb_img = batch["rgb_img"].cuda()
      depth_img = batch["depth_img"].cuda()
      pose_4x4 = batch["pose_4x4"].cuda()
      if self.cfg.depth_limit >= 0:
        depth_img[torch.logical_and(
          torch.isfinite(depth_img),
          depth_img > self.cfg.depth_limit)] = torch.inf

      kwargs = dict()
      if "confidence_map" in batch.keys():
        kwargs["conf_map"] = batch["confidence_map"].cuda()

      if "GTEncoder" in self.cfg.encoder._target_:
        kwargs["feat_img"] = torch.nn.functional.one_hot(
          batch["semseg_img"].cuda(),
          self.num_classes).squeeze(1).permute(0, 3, 1, 2).float()

      if self.vis is not None:
        if i % self.cfg.vis.pose_period == 0:
          self.vis.log_pose(batch["pose_4x4"][-1])
        if i % self.cfg.vis.input_period == 0:
          self.vis.log_img(batch["rgb_img"][-1].permute(1,2,0))
          self.vis.log_depth_img(depth_img.cpu()[-1].squeeze())
          if "confidence_map" in batch.keys():
            self.vis.log_img(batch["confidence_map"][-1])
          if "semseg_img" in batch.keys():
            self.vis.log_label_img(batch["semseg_img"][-1])

      r = mapper.process_posed_rgbd(rgb_img, depth_img, pose_4x4, **kwargs)

      if self.vis is not None:
        if i % self.cfg.vis.input_period == 0:
          mapper.vis_update(**r)
          if self.cfg.vis_2d_semseg:
            B, C, H, W = r["feat_img"].shape
            flat_feat_img = r["feat_img"].permute(0, 2, 3, 1).reshape(-1, C)
            semseg_img = eval_utils.compute_semseg_preds(
              flat_feat_img, text_embeds,
              self.cfg.prompt_denoising_thresh,
              self.cfg.prediction_thresh, self.cfg.chunk_size)
            self.vis.log_label_img(semseg_img.reshape(B, H, W), layer="img_pred")
        if i % self.cfg.vis.map_period == 0:
          mapper.vis_map()

        self.vis.step()

      yield mapper.global_vox_xyz, mapper.global_vox_feat

    mapper.accum_semantic_voxels()
    return mapper.global_vox_xyz, mapper.global_vox_feat

  def load_external_preds(self):
    """Load external semantic segmentation predictions from a file."""
    fn = os.path.join(self.cache_scene_dir, "external_semseg_pred.pt")
    if not os.path.exists(fn):
      logger.fatal("Could not find external prediction from '%s'", fn)
      sys.exit(1)
    logger.info("Loading external prediction 3D voxels from '%s'...", fn)
    d = torch.load(fn, weights_only=True)
    pred_xyz = d["semseg_pred_xyz"].to(self.device)
    pred_label = d["semseg_pred_label"].to(self.device)
    pred_label = self.dataset._cat_id_to_cat_index[pred_label]
    pred_onehot = torch.nn.functional.one_hot(pred_label, self.num_classes)
    pred_xyz, pred_onehot = g3d.pointcloud_to_sparse_voxels(
      pred_xyz, self.cfg.mapping.vox_size, pred_onehot, aggregation="sum")
    pred_label = torch.argmax(pred_onehot, dim=-1)
    return pred_xyz, pred_label

  def compute_semseg_metrics(self,
                             gt: torch.LongTensor,
                             preds: torch.LongTensor,
                             gt_xyz: torch.FloatTensor = None,
                             preds_xyz: torch.FloatTensor = None):
    """Compute semantic segmentation metrics.

    Args:
      gt: N Long tensor of ground truth labels
      preds: N Long tensor of predicted labels
      gt_xyz: Nx3 float tensor of ground truth labels coordinates
      preds_xyz: Nx3 float tensor of predicted labels coordinates

    Returns:
      dict of metrics
        - tp: Long Tensor of true positives for each class
        - fp: Long Tensor of false positives for each class
        - fn: Long Tensor of false negatives for each class
        - tn: Long Tensor of true negatives for each class
        - iou: Float Tensor of IoU for each class
        - fiou: Float Tensor frequency weighted IoU for each class
        - miou: mean IoU
        - fmiou: mean frequency weighted IoU
        - acc: Accuracy
    """
    if gt_xyz is not None:
      assert preds_xyz is not None
      # If gt and feat coords were passed that means we must align the voxels.
      if self.cfg.k > 0:
        gt, aligned_preds = eval_utils.align_labels_with_knn(
          gt_xyz, gt, preds_xyz, preds, k=self.cfg.k)
      else:
        gt, aligned_preds = eval_utils.align_labels_with_vox_grid(
          gt_xyz, gt, preds_xyz, preds, self.cfg.mapping.vox_size)
    else:
      aligned_preds = preds

    tp, fp, fn, tn = eval_utils.eval_gt_pred(
      aligned_preds, gt, num_classes=self.num_classes)
    iou = tp / (tp+fp+fn)
    freq = tp+fn
    # Ignore IoU of classes not present in the scene
    # Even with a powerful segmentor these will always be 0 or NaN.
    iou[freq == 0] = torch.nan
    freq_weight = freq / torch.sum(freq)
    fiou = freq_weight * iou
    miou = torch.nanmean(iou)
    fmiou = torch.nansum(fiou)
    acc = torch.mean((aligned_preds[gt != 0] == gt[gt!=0]).float())

    m = dict(tp=tp, fp=fp, fn=fn, tn=tn, iou=iou, fiou=fiou, miou=miou,
             fmiou=fmiou, acc=acc)
    return m

  def predict_and_semseg_eval(self, semseg_gt_xyz, semseg_gt_label,
                              feats_xyz, feats_feats, text_embeds,
                              vis=True):
    if (self.feat_compressor is not None and 
        not self.cfg.querying.compressed):
      feats_feats = self.feat_compressor.decompress(feats_feats)
    feats_lang_feats = \
      self.encoder.align_spatial_features_with_language(
        feats_feats.unsqueeze(-1).unsqueeze(-1)).squeeze(-1).squeeze(-1)
    semseg_pred_xyz = feats_xyz
    semseg_pred_label = eval_utils.compute_semseg_preds(
      feats_lang_feats, text_embeds,
      self.cfg.prompt_denoising_thresh,
      self.cfg.prediction_thresh, self.cfg.chunk_size)
    m = self.compute_semseg_metrics(
      semseg_gt_label, semseg_pred_label, semseg_gt_xyz, semseg_pred_xyz)

    if vis and self.vis is not None:
      self.vis.log_label_pc(semseg_pred_xyz, semseg_pred_label,
                            layer="predictions/voxels")
    return m

  def log_metrics(self, i, m: dict[str, Union[float, torch.Tensor]], 
                  prefix: str = "semseg"):
    """Logs and visualizes Semantic Segmentation Metrics
    
    Args:
      i: Current step.
      m: Dictionary of the format:
        {<metric_name>: <metric_value>}
        <metric_value> can either be a single value for an aggregate over
        classes or C values, where C is the number of classes.
    """
    aggregate_metrics = [k for k,v in m.items() if v.dim() == 0]
    msg = list()
    for met in aggregate_metrics:
      msg.append(f"{met}={m[met].item():.4f}")
    logger.info("[%4d] %s", i, ", ".join(msg))

    if self.vis is not None:
      for met in aggregate_metrics:
        rr.log(f"metrics/{prefix}/summary/{met}", rr.Scalar(m[met].item()))

      class_wise_metrics = [k for k,v in m.items() if v.dim() > 0]
      for i, cls in enumerate(self.dataset._cat_index_to_name[1:]):
        cls = cls.replace(" ", "_")
        for met in class_wise_metrics:
          rr.log(f"metrics/{prefix}/classwise/{i+1}_{cls}/{met}",
                rr.Scalar(m[met][i].item()))

  def save_metrics(self,
                   m: OrderedDict[dict[str, Union[float, torch.Tensor]]],
                   prefix: str = "semseg"):

    """Save the results of the evaluation in the scene directory.

    Saves the following:
    A. Final metrics for each class as a csv file.
    B. Final metrics over all classes as a csv file. In addition to saving to
       the scene directory, it will also save to the dataset directory merging
       the results from all scenes in a single csv file.

    if len(m) > 1, saves the following:
    C. Average metrics over all classes for each evaluation step as a csv file.
    D. Metrics for each class for each evaluation step. This is stored as 
       a directory which contains a csv file for each class. The csv file has
       the metrics for that class at each evaluation step.
  
    Args:
      m: Ordered dictionary of the format:
        {<step>: {<metric_name>: <metric_value>}}
        <metric_value> can either be a single value for an aggregate over
        classes or C values, where C is the number of classes.
    """
    if not self.store_output:
      return

    last_m = next(reversed(m.values()))

    ## A. Store final metrics for each class.
    class_wise_metrics = [k for k,v in last_m.items() if v.dim() > 0]
    csv_header = "index,label," + ",".join(class_wise_metrics) + "\n"
    csv_lines = [csv_header]

    for i in range(self.num_classes-1): # -1 for the ignore class
      fields = [
        str(i+1),
        str(self.dataset._cat_index_to_name[i+1]),
      ]
      fields.extend([str(last_m[k][i].item()) for k in class_wise_metrics])
      csv_lines.append(",".join(fields) + "\n")
    results_file_name = os.path.join(self.cache_scene_dir,
                                     f"{prefix}_final_class_results.csv")
    with open(results_file_name, "w") as f:
      f.writelines(csv_lines)

    ## B. Summary metrics for all classes.
    aggregate_metrics = [k for k,v in last_m.items() if v.dim() == 0]
    csv_header = "scene_name," + ",".join(aggregate_metrics) + "\n"
    csv_lines = [csv_header]
    fields = [self.dataset.scene_name]
    fields.extend([str(last_m[k].item()) for k in aggregate_metrics])
    csv_lines.append(",".join(fields) + "\n")
    results_file_name = os.path.join(self.cache_scene_dir,
                                     f"{prefix}_final_summary_results.csv")
    with open(results_file_name, "w") as f:
      f.writelines(csv_lines)

    results_file_name = os.path.join(self.cfg.eval_out,
                                     self.dataset.__class__.__name__,
                                     f"{prefix}_final_summary_results.csv")
    # Store in the dataset directory too merging with other scene results if
    # they exist.
    csv_lines = [csv_header]
    field_line = ",".join(fields) + "\n"
    if os.path.exists(results_file_name):
      with open(results_file_name, "r") as f:
        csv_lines = f.readlines()
      for i, l in enumerate(csv_lines):
        if self.dataset.scene_name in l:
          csv_lines[i] = field_line
          break
      else:
        csv_lines.append(field_line)
    else:
      csv_lines.append(field_line)
    with open(results_file_name, "w") as f:
      f.writelines(csv_lines)

    if len(m) <= 1: # No online evaluation
      return

    ## C. Average metrics over all classes for each evaluation step
    csv_header = "step," + ",".join(aggregate_metrics) + "\n"
    csv_lines = [csv_header]
    for step, mm in m.items():
      fields = [str(step)] + [str(mm[k].item()) for k in aggregate_metrics]
      csv_lines.append(",".join(fields) + "\n")
    results_file_name = os.path.join(self.cache_scene_dir,
                                     f"{prefix}_online_summary_results.csv")
    with open(results_file_name, "w") as f:
      f.writelines(csv_lines)

    ## D. Metrics for each class for each evaluation step.
    results_dir = os.path.join(
      self.cache_scene_dir, f"{prefix}_online_classwise_results")
    if os.path.exists(results_dir):
      shutil.rmtree(results_dir)
    os.makedirs(results_dir)
    for i in range(self.num_classes-1):
      csv_header = ["step"] + class_wise_metrics
      csv_header = ",".join(csv_header) + "\n" + os.linesep
      csv_lines = [csv_header]
      for step, mm in m.items():
        fields = [str(step)]
        fields += [str(mm[k][i].item()) for k in class_wise_metrics]
        csv_lines.append(",".join(fields) + "\n")

      results_file_name = os.path.join(
        results_dir,
        f"{i+1}_{self.dataset._cat_index_to_name[i+1]}.csv")
      with open(results_file_name, "w") as f:
        f.writelines(csv_lines)

  def run(self):
    ## 1. Get Semantic Segmentation Ground Truth
    eval_utils.reset_seed(self.cfg.seed)
    if self.cfg.load_external_gt:
      semseg_gt_xyz, semseg_gt_label = self.load_externel_semseg_gt()
    else:
      if self.cache_is_valid("semseg_gt"):
        logger.info("Loading cached ground truth 3D voxels...")
        d = torch.load(self.cache_file_paths["semseg_gt"], weights_only=True)
        semseg_gt_xyz = d["semseg_gt_xyz"].to(self.device)
        semseg_gt_label = d["semseg_gt_label"].to(self.device)
      else:
        semseg_gt_xyz, semseg_gt_label = self.compute_semseg_gt()
        logger.info("Ground truth voxel labels generated.")
        gt_file_name = self.cache_file_paths["semseg_gt"]
        if self.store_output and not self.cfg.load_external_gt:
          os.makedirs(os.path.dirname(gt_file_name), exist_ok=True)
          torch.save(dict(semseg_gt_xyz=semseg_gt_xyz,
                          semseg_gt_label=semseg_gt_label), gt_file_name)

    if self.vis is not None:
      self.vis.log_label_pc(semseg_gt_xyz, semseg_gt_label,
                            layer="ground_truth")

    results_dict = OrderedDict()
    if self.cfg.load_external_pred:
      semseg_pred_xyz, semseg_pred_label = self.load_external_preds()
      if self.vis is not None:
        self.vis.log_label_pc(semseg_pred_xyz, semseg_pred_label,
                              layer="predictions")
      m = self.compute_semseg_metrics(semseg_gt_label, semseg_pred_label,
                                      semseg_gt_xyz, semseg_pred_xyz)
      self.log_metrics(-1, m)
      results_dict[-1] = m
      self.save_metrics(results_dict)
      OmegaConf.save(self.cfg, self.cache_cfg_fn)
      return

    encoder_kwargs = dict()
    if "NARadioEncoder" in self.cfg.encoder._target_:
      encoder_kwargs["input_resolution"] = (self.dataset.rgb_h,
                                            self.dataset.rgb_w)
    if "classes" in self.cfg.encoder:
      encoder_kwargs["classes"] = self.dataset.cat_index_to_name[1:]
    self.encoder = hydra.utils.instantiate(self.cfg.encoder, **encoder_kwargs)

    self.feat_compressor = None
    if ("feat_compressor" in self.cfg.mapping and
        self.cfg.mapping.feat_compressor is not None):
      self.feat_compressor = hydra.utils.instantiate(
        self.cfg.mapping.feat_compressor)
    
    ## 2. Compute text embeddings.
    if self.cache_is_valid("text_embeds"):
      logger.info("Loading cached text embeddings...")
      d = torch.load(self.cache_file_paths["text_embeds"], weights_only=True)
      text_embeds = d["text_embeds"].to(self.device)
    else:
      text_embeds = self.compute_text_embeds()
      if self.store_output:
        torch.save(dict(text_embeds=text_embeds),
                   self.cache_file_paths["text_embeds"])

    ## 3. Compute Map features.
    eval_utils.reset_seed(self.cfg.seed)
    results_dict = dict()
    if self.cache_is_valid("map") and self.cfg.online_eval_period <= 0:
      logger.info("Loading cached map features...")
      d = torch.load(self.cache_file_paths["map"], weights_only=True)
      feats_xyz = d["feats_xyz"].to(self.device)
      feats_feats = d["feats_feats"].to(self.device)
      i = -1
    else:
      i = 0

      mapper = hydra.utils.instantiate(
        self.cfg.mapping, encoder=self.encoder,
        intrinsics_3x3=self.dataset.intrinsics_3x3, visualizer=self.vis,
        feat_compressor=self.feat_compressor)
      for feats_xyz, feats_feats in self.mapping_loop(mapper, text_embeds):
        if (i != 0 and self.cfg.online_eval_period > 0 and
            i % self.cfg.online_eval_period == 0):
          results_dict[i] = self.predict_and_semseg_eval(
            semseg_gt_xyz, semseg_gt_label,
            feats_xyz, feats_feats, text_embeds)
          self.log_metrics(i, results_dict[i])

        i += 1

      map_file_name = self.cache_file_paths["map"]
      if self.store_output:
        os.makedirs(os.path.dirname(map_file_name), exist_ok=True)
        torch.save(dict(feats_xyz=feats_xyz, feats_feats=feats_feats),
                   map_file_name)

    if self.vis is not None:
      self.vis.log_feature_pc(feats_xyz, feats_feats, layer="map_features_pca")

    # 4. Predict and evaluate semantic segmentation
    m = self.predict_and_semseg_eval(
      semseg_gt_xyz, semseg_gt_label,
      feats_xyz, feats_feats, text_embeds)
    self.log_metrics(i, m)
    results_dict[i] = m

    # 5. Save the results
    self.save_metrics(results_dict)

    OmegaConf.save(self.cfg, self.cache_cfg_fn)


@hydra.main(version_base="1.2",
            config_path="../rayfronts/configs",
            config_name="default")
@torch.inference_mode()
def main(cfg=None):
  semseg_eval = SemSegEval(cfg)
  semseg_eval.run()

if __name__ == "__main__":
  main()
